
<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://natsunoshion.github.io/posts/%E7%AC%AC9%E8%AF%BE%EF%BC%9AAttention%E5%92%8CTransformer/">
      
      
        <link rel="prev" href="../%E7%AC%AC8%E8%AF%BE%EF%BC%9ARNN/">
      
      
        <link rel="next" href="../%E7%AC%AC10%E8%AF%BE%EF%BC%9A%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.5.3, mkdocs-material-9.4.2">
    
    
      
        <title>第9课：Attention和Transformer - natsunoshion 的个人博客</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.d451bc0e.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.7/katex.min.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
   <link href="../../assets/stylesheets/glightbox.min.css" rel="stylesheet"/><style>
        html.glightbox-open { overflow: initial; height: 100%; }
        .gslide-title { margin-top: 0px; user-select: text; }
        .gslide-desc { color: #666; user-select: text; }
        .gslide-image img { background: white; }
        
            .gscrollbar-fixer { padding-right: 15px; }
            .gdesc-inner { font-size: 0.75rem; }
            body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color);}
            body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color);}
            body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color);}
            </style> <script src="../../assets/javascripts/glightbox.min.js"></script></head>
  
  
    <body dir="ltr">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#rnn" class="md-skip">
          跳转至
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="页眉">
    <a href="../.." title="natsunoshion 的个人博客" class="md-header__button md-logo" aria-label="natsunoshion 的个人博客" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            natsunoshion 的个人博客
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              第9课：Attention和Transformer
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="搜索" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="查找">
        
        <button type="reset" class="md-search__icon md-icon" title="清空当前内容" aria-label="清空当前内容" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            正在初始化搜索引擎
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="导航栏" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="natsunoshion 的个人博客" class="md-nav__button md-logo" aria-label="natsunoshion 的个人博客" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    natsunoshion 的个人博客
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    主页
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  <span class="md-ellipsis">
    CS231n 学习
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            CS231n 学习
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../%E7%AC%AC2%E8%AF%BE%EF%BC%9AKNN%20%E4%B8%8E%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    第2课：KNN 与线性分类
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../%E7%AC%AC3%E8%AF%BE%EF%BC%9A%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%8E%E4%BC%98%E5%8C%96%E5%99%A8/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    第3课：损失函数与优化器
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../%E7%AC%AC4%E8%AF%BE%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BB%8B%E7%BB%8D/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    第4课：神经网络介绍
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../%E7%AC%AC5%E8%AF%BE%EF%BC%9A%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    第5课：卷积神经网络
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../%E7%AC%AC6%E8%AF%BE%EF%BC%9A%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%881%EF%BC%89/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    第6课：训练神经网络（1）
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../%E7%AC%AC7%E8%AF%BE%EF%BC%9A%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%882%EF%BC%89/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    第7课：训练神经网络（2）
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../%E7%AC%AC8%E8%AF%BE%EF%BC%9ARNN/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    第8课：RNN
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    第9课：Attention和Transformer
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    第9课：Attention和Transformer
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#rnn" class="md-nav__link">
    基于 RNN 的注意力机制
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    普遍注意力层
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#transformer" class="md-nav__link">
    Transformer
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../%E7%AC%AC10%E8%AF%BE%EF%BC%9A%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    第10课：视频理解
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../%E7%AC%AC11%E8%AF%BE%EF%BC%9A%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%8E%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    第11课：目标检测与图像分割
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../CNN%E6%9E%B6%E6%9E%84/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    CNN架构
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../%EF%BC%88%E8%BF%99%E4%B8%80%E8%AF%BE%E8%A2%AB%E5%88%A0%E6%8E%89%E4%BA%86%EF%BC%89%E7%AC%AC8%E8%AF%BE%EF%BC%9A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BD%AF%E4%BB%B6/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    （这一课被删掉了）第8课：深度学习软件
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="">
            
  
  <span class="md-ellipsis">
    论文阅读
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            论文阅读
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ALighting-Every-Darkness-in-Two-Pairs-A-Calibration-Free-Pipeline-for-RAW-Denoising/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Lighting-Every-Darkness-in-Two-Pairs-A-Calibration-Free-Pipeline-for-RAW-Denoising
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ANuI-Go-Recursive-Non-Local-Encoder-Decoder-Network-for-Retinal-Image-Non-Uniform-Illumination-Removal/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    NuI-Go-Recursive-Non-Local-Encoder-Decoder-Network-for-Retinal-Image-Non-Uniform-Illumination-Removal
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#rnn" class="md-nav__link">
    基于 RNN 的注意力机制
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    普遍注意力层
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#transformer" class="md-nav__link">
    Transformer
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  <h1>第9课：Attention和Transformer</h1>

<blockquote>
<p><a class="glightbox" href="https://i.imgur.com/qUIUOWD.png" data-type="image" data-width="100%" data-height="auto" data-desc-position="bottom"><img alt="" src="https://i.imgur.com/qUIUOWD.png" /></a></p>
<p>可以看出，RNN 有很多问题，而 Transformer 没有那么多层。</p>
</blockquote>
<h2 id="rnn">基于 RNN 的注意力机制<a class="headerlink" href="#rnn" title="Permanent link">&para;</a></h2>
<p><a class="glightbox" href="https://i.imgur.com/ISp6JRG.png" data-type="image" data-width="100%" data-height="auto" data-desc-position="bottom"><img alt="" src="https://i.imgur.com/ISp6JRG.png" /></a></p>
<p>除了 $h_0$ 以外还需要原图像：<strong>增加信息量</strong>。（图像的特征）</p>
<p><a class="glightbox" href="https://i.imgur.com/HZajgfp.png" data-type="image" data-width="100%" data-height="auto" data-desc-position="bottom"><img alt="" src="https://i.imgur.com/HZajgfp.png" /></a></p>
<p>在 Attention 中还可以进行特定的调整，例如强调图片中心的话, 那可以限定 $a_{1,1,1}$ 较大。</p>
<p>注意力机制的可视化：</p>
<p><a class="glightbox" href="https://i.imgur.com/g2P73oR.png" data-type="image" data-width="100%" data-height="auto" data-desc-position="bottom"><img alt="" src="https://i.imgur.com/g2P73oR.png" /></a></p>
<p>可视化涉及到上采样，所以图片会变得模糊。（上采样是因为前面的橙色矩阵经过了下采样，较小）</p>
<p><a class="glightbox" href="https://i.imgur.com/SB3IHtW.png" data-type="image" data-width="100%" data-height="auto" data-desc-position="bottom"><img alt="" src="https://i.imgur.com/SB3IHtW.png" /></a></p>
<p>数据集中上下文可能带来偏差，有论文引入性别 Loss：（这个在 2023 的 PPT 里被删掉了，同时因为我看的网课是 21 版的，所以又换成 21 年的 PPT 了）</p>
<p><a class="glightbox" href="https://i.imgur.com/zfWbST0.png" data-type="image" data-width="100%" data-height="auto" data-desc-position="bottom"><img alt="" src="https://i.imgur.com/zfWbST0.png" /></a></p>
<p>下面是原始版本的机翻和 Attention 版本的机翻：</p>
<p><a class="glightbox" href="https://i.imgur.com/5EOVW4o.png" data-type="image" data-width="100%" data-height="auto" data-desc-position="bottom"><img alt="" src="https://i.imgur.com/5EOVW4o.png" /></a></p>
<p><a class="glightbox" href="https://i.imgur.com/dHAPFXA.png" data-type="image" data-width="100%" data-height="auto" data-desc-position="bottom"><img alt="" src="https://i.imgur.com/dHAPFXA.png" /></a></p>
<p>以下结果：</p>
<p><a class="glightbox" href="https://i.imgur.com/W1suRX7.png" data-type="image" data-width="100%" data-height="auto" data-desc-position="bottom"><img alt="" src="https://i.imgur.com/W1suRX7.png" /></a></p>
<p>说明了有了注意力机制，识别不是顺序的。</p>
<h2 id="_1">普遍注意力层<a class="headerlink" href="#_1" title="Permanent link">&para;</a></h2>
<p><a class="glightbox" href="https://i.imgur.com/odAVlpU.png" data-type="image" data-width="100%" data-height="auto" data-desc-position="bottom"><img alt="" src="https://i.imgur.com/odAVlpU.png" /></a></p>
<p>Alignment 的含义可以说是计算相似度。</p>
<p>更加通用的形式：（变为输入向量和查询向量）</p>
<p><a class="glightbox" href="https://i.imgur.com/vKE0Pzu.png" data-type="image" data-width="100%" data-height="auto" data-desc-position="bottom"><img alt="" src="https://i.imgur.com/vKE0Pzu.png" /></a></p>
<p>这里点积后要除一个 $\sqrt D$，是为了防止值过大，在 Softmax 归一化后有一个 a 的值几乎为 1（维度大了之后，点积结果很大很大，方差很大，最后 Softmax 的熵很低）。这里的 $\sqrt D$  和方差有关。</p>
<p>更多的查询向量：</p>
<p><a class="glightbox" href="https://i.imgur.com/rBVeNyd.png" data-type="image" data-width="100%" data-height="auto" data-desc-position="bottom"><img alt="" src="https://i.imgur.com/rBVeNyd.png" /></a></p>
<p>注意到，输入向量（左边紫色）被用了两次，表达能力可能受限。我们希望用线性变换让它变成不同的值，增加模型的能力：（两个方向有不同的适配性，Identity Mapping）</p>
<p><a class="glightbox" href="https://i.imgur.com/sRV9oL6.png" data-type="image" data-width="100%" data-height="auto" data-desc-position="bottom"><img alt="" src="https://i.imgur.com/sRV9oL6.png" /></a></p>
<p>需要注意，q 的个数和 x 的个数可以不一样。（Sequence to Sequence）</p>
<p><strong>自注意力机制</strong>：</p>
<p><a class="glightbox" href="https://i.imgur.com/W5mTWJS.png" data-type="image" data-width="100%" data-height="auto" data-desc-position="bottom"><img alt="" src="https://i.imgur.com/W5mTWJS.png" /></a></p>
<p>上图中的 q 是从输入向量得到的，这就是自注意力机制。</p>
<p>可以看出，与排序是无关的：</p>
<p><a class="glightbox" href="https://i.imgur.com/RYIsK0h.png" data-type="image" data-width="100%" data-height="auto" data-desc-position="bottom"><img alt="" src="https://i.imgur.com/RYIsK0h.png" /></a></p>
<p>也就是说现在无论输入是什么顺序，最终得到的顺序都是一样的（即将该序列打乱后并不影响输出结果）。</p>
<p><em>pos</em>(.) 的需求：</p>
<ol>
<li>它应该为每个时间步长（单词在句子中的位置）输出唯一的编码</li>
<li>任意两个时间步长之间的距离在不同长度的句子中应该一致。</li>
<li>我们的模型应该无需任何努力就能推广到更长的句子。它的值应该是有界的。
4.它必须是确定性的。</li>
</ol>
<p>对位置进行编码：</p>
<p><a class="glightbox" href="https://i.imgur.com/fCf9au1.png" data-type="image" data-width="100%" data-height="auto" data-desc-position="bottom"><img alt="" src="https://i.imgur.com/fCf9au1.png" /></a></p>
<p>注意一下，这里的 sin 和 cos 是两个不同的矩阵元素。</p>
<p>最低位周期最短。</p>
<p>通过计算，可以发现 $||(p(t)-p(t-1))||_2$ 为仅与 $w$ 有关的常数。</p>
<p><a class="glightbox" href="https://i.imgur.com/dFLisO0.png" data-type="image" data-width="100%" data-height="auto" data-desc-position="bottom"><img alt="" src="https://i.imgur.com/dFLisO0.png" /></a></p>
<p>补充一下，这里有点难的，这里其实可以理解为由原来的 $y_0=f(x_0)$ 变为了 $y_0=f(x_0, p_0)$，多了一个位置参数。</p>
<p>为了不接受到未来的知识（例如输入有顺序 $x_0,x_1,x_2$，那么不想让 $y_0$ 收到 $x_1,x_2$ 的信息 $y_1$ 也类似），那么可以在 Alignment 中置为 $-\inf$，这样 Softmax 之后就成 0 了：</p>
<p><a class="glightbox" href="https://i.imgur.com/H9zzKn9.png" data-type="image" data-width="100%" data-height="auto" data-desc-position="bottom"><img alt="" src="https://i.imgur.com/H9zzKn9.png" /></a></p>
<p>MHA 或叫做 MSA：</p>
<p><a class="glightbox" href="https://i.imgur.com/f3Vs31v.png" data-type="image" data-width="100%" data-height="auto" data-desc-position="bottom"><img alt="" src="https://i.imgur.com/f3Vs31v.png" /></a></p>
<p>有点像分段并行，中间各自会进行非线性变换，最后会 Concat 在一起。</p>
<p>好处：</p>
<ul>
<li>有利于学习特征向量的细节。</li>
<li>输入有顺序 $x_0, x_1, x_2$，相当于把 Channel 分为了三段（假设 $x_i$ 都是 768 大小，也就是 $32\times 32$）即 Feature Map <strong>分组</strong>，各组做自己的一些操作，做完之后再合并为大的 Feature Map。</li>
</ul>
<p><a class="glightbox" href="https://i.imgur.com/FKHkSvO.png" data-type="image" data-width="100%" data-height="auto" data-desc-position="bottom"><img alt="" src="https://i.imgur.com/FKHkSvO.png" /></a></p>
<p>RNN 和 Transformer 比较：</p>
<ul>
<li>RNNs：<ul>
<li>(+) LSTMs 在处理长序列时效果不错。  </li>
<li>(-) 需要一个有序的输入序列  </li>
<li>(-) 顺序计算:后续隐藏状态只能在计算前一个状态结束后计算。</li>
</ul>
</li>
<li>Transformers:  <ul>
<li>(+) 在长序列上效果很好。每个注意力计算考虑所有输入。  </li>
<li>(+) 可以处理无序集合或者通过位置编码处理有序序列。  </li>
<li>(+) 并行计算: 所有输入的对齐和注意力得分都可以并行计算。  </li>
<li>(-) 需要大量内存: 对一个单独的自注意力头来说, 需要计算和存储 N x M 个对齐和注意力得分。(但是 GPU 性能不断提高)</li>
</ul>
</li>
</ul>
<h2 id="transformer">Transformer<a class="headerlink" href="#transformer" title="Permanent link">&para;</a></h2>
<p><a class="glightbox" href="https://i.imgur.com/dvuYGgo.png" data-type="image" data-width="100%" data-height="auto" data-desc-position="bottom"><img alt="" src="https://i.imgur.com/dvuYGgo.png" /></a></p>
<p>编码：</p>
<p><a class="glightbox" href="https://i.imgur.com/ueSonRx.png" data-type="image" data-width="100%" data-height="auto" data-desc-position="bottom"><img alt="" src="https://i.imgur.com/ueSonRx.png" /></a></p>
<ul>
<li>Layer norm 可以减弱聚焦。</li>
<li>Position encoding 只需要一次。（一次破坏位置不变性就行）（Position encoding 需要有一致性、确定性）</li>
</ul>
<p>解码：</p>
<p><a class="glightbox" href="https://i.imgur.com/iSbFT38.png" data-type="image" data-width="100%" data-height="auto" data-desc-position="bottom"><img alt="" src="https://i.imgur.com/iSbFT38.png" /></a></p>
<p>Masked：防止看见未来。（<strong>有争议</strong>，没有必要 Mask，因为本来就是递归生成的，不存在信息泄露）</p>
<p>Multi-head：不是自注意力，它跨越了 encoder 的输出。</p>
<p>实际上是先由 [start] 得到 person，接下来由 [[start], person] 共同得到 [person, wearing]，而不是由 person 直接得到 wearing。（不是图上的 4 个进去 4 个出来）</p>
<p>甚至可以去除 CNN：</p>
<p><a class="glightbox" href="https://i.imgur.com/d2Mxcwq.png" data-type="image" data-width="100%" data-height="auto" data-desc-position="bottom"><img alt="" src="https://i.imgur.com/d2Mxcwq.png" /></a></p>
<p>Transformer 被大规模应用：</p>
<p><a class="glightbox" href="https://i.imgur.com/NepUum1.png" data-type="image" data-width="100%" data-height="auto" data-desc-position="bottom"><img alt="" src="https://i.imgur.com/NepUum1.png" /></a></p>
<p>总结：</p>
<p><a class="glightbox" href="https://i.imgur.com/tN6Fi4d.png" data-type="image" data-width="100%" data-height="auto" data-desc-position="bottom"><img alt="" src="https://i.imgur.com/tN6Fi4d.png" /></a></p>
<ol>
<li>Attention 让 RNN 强大了许多，不需要 bottleneck。</li>
<li>通用 attention layer 可以插入很多地方。</li>
<li>Transformer 非常强大。（使用 self-attention 和 layer norm）</li>
</ol>





                
              </article>
            </div>
          
          
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.sections"], "search": "../../assets/javascripts/workers/search.a264c092.min.js", "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.726fbb30.min.js"></script>
      
        <script src="../../javascripts/katex.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.7/katex.min.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.7/contrib/auto-render.min.js"></script>
      
    
  <script>document$.subscribe(() => {const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});})</script></body>
</html>